{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "805241d8",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ed0a7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "import zipfile\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "import torchaudio\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b535a5c9",
   "metadata": {},
   "source": [
    "# Data manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3f4155",
   "metadata": {},
   "source": [
    "Now we are gonna download and manipulate the data to make them usable for the dataset that we are gonna create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631ec657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inizio download del file da Google Drive...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train.zip: 0.00B [37:45, ?B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download completato!\n",
      "Estrazione di train.zip nella cartella 'train_data'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Estrazione file: 100%|██████████| 31994/31994 [02:07<00:00, 251.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estrazione completata!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# === CONFIGURAZIONE ===\n",
    "FILE_ID = \"1zHPs5LurXfPwDDD2jXhpegEchvXTQWRP\"  # <-- Cambia qui se serve\n",
    "OUTPUT_ZIP = \"train.zip\"\n",
    "EXTRACT_DIR = \"train_data\"\n",
    "\n",
    "# === 1. Scarica da Google Drive con barra di progresso ===\n",
    "print(\"Inizio download del file da Google Drive...\")\n",
    "\n",
    "# Usa gdown per scaricare con URL\n",
    "url = f\"https://drive.google.com/uc?id={FILE_ID}\"\n",
    "\n",
    "# Usa una funzione di hook per mostrare la barra\n",
    "def tqdm_hook(t):\n",
    "    last_b = [0]\n",
    "\n",
    "    def update_to(b=1, bsize=1, tsize=None):\n",
    "        if tsize is not None:\n",
    "            t.total = tsize\n",
    "        t.update((b - last_b[0]) * bsize)\n",
    "        last_b[0] = b\n",
    "    return update_to\n",
    "\n",
    "with tqdm(unit='B', unit_scale=True, desc=OUTPUT_ZIP, miniters=1) as t:\n",
    "    gdown.download(url, OUTPUT_ZIP, quiet=True, use_cookies=False)\n",
    "\n",
    "print(\"Download completato!\")\n",
    "\n",
    "# === 2. Estrazione del file ZIP ===\n",
    "print(f\"Estrazione di {OUTPUT_ZIP} nella cartella '{EXTRACT_DIR}'...\")\n",
    "\n",
    "# Crea cartella se non esiste\n",
    "if not os.path.exists(EXTRACT_DIR):\n",
    "    os.makedirs(EXTRACT_DIR)\n",
    "\n",
    "with zipfile.ZipFile(OUTPUT_ZIP, 'r') as zip_ref:\n",
    "    # tqdm per mostrare il progresso dei file estratti\n",
    "    for file in tqdm(zip_ref.namelist(), desc=\"Estrazione file\"):\n",
    "        zip_ref.extract(file, EXTRACT_DIR)\n",
    "\n",
    "print(\"Estrazione completata!\")\n",
    "\n",
    "os.remove(OUTPUT_ZIP)  # Rimuovi il file ZIP dopo l'estrazione\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297e68cc",
   "metadata": {},
   "source": [
    "Now we'll create a Dataframe with the following structure:\n",
    "audio_PATH, score, audio_PATH\n",
    "\n",
    "Our \"degradation_points.txt\" has the points decided on the type of noise that has been applied on the original audio (0: small noise, 1.0: big noise).\n",
    "\n",
    "Our goal is to create a model that assign a quality score to input audio (0: bad_quality, 1: very good quality). So to understand the idea behind the assigned score we have to consider 2 different cases:\n",
    "- (clean, degraded): the final score will be 1.0 - score, cause if we had applied big noise the initial score will be almost 1, so the final quality score will be near 0.\n",
    "- (degraded, clean): the final quality score will be equal to 1.0 cause clean represent the optimal quality that has to be reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3faa1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a csv file containing all the necessary information\n",
    "clean_dir = os.path.join(EXTRACT_DIR, 'train', 'clean')\n",
    "degraded_dir = os.path.join(EXTRACT_DIR, 'train', 'degraded')\n",
    "score_file = os.path.join(EXTRACT_DIR, 'degradation_points.txt')\n",
    "\n",
    "# Load score file\n",
    "df = pd.read_csv(score_file, sep='\\t', header=None, names=['filename', 'score'])\n",
    "# Add clean_path on csv\n",
    "df['clean_path'] = df['filename'].apply(lambda x: os.path.join(clean_dir, x.split('/')[-1].split('_')[0] + '.mp3'))\n",
    "df['score'] = df['score'].astype(float)\n",
    "df['degraded_path'] = df['filename'].apply(lambda x: os.path.join(degraded_dir, x.split('/')[-1]))\n",
    "\n",
    "df = df[[\"clean_path\", \"score\", \"degraded_path\"]]\n",
    "\n",
    "# Coppie (clean → degraded): il degraded è il target, qualità = 1 - score\n",
    "forward = pd.DataFrame({\n",
    "    \"source_path\": df[\"clean_path\"],\n",
    "    \"target_path\": df[\"degraded_path\"],\n",
    "    \"quality_score\": 1.0 - df[\"score\"]\n",
    "})\n",
    "\n",
    "# Coppie (degraded → clean): il clean è il target, qualità massima = 1.0\n",
    "backward = pd.DataFrame({\n",
    "    \"source_path\": df[\"degraded_path\"],\n",
    "    \"target_path\": df[\"clean_path\"],\n",
    "    \"quality_score\": 1.0\n",
    "})\n",
    "\n",
    "# Combina\n",
    "paired_df = pd.concat([forward, backward], ignore_index=True)\n",
    "\n",
    "# Visualizza esempio\n",
    "# print(paired_df.head())\n",
    "\n",
    "# Salva se vuoi\n",
    "paired_df.to_csv(\"train_data/paired_audio_quality_dataset.csv\", index=False, header=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d20afe4",
   "metadata": {},
   "source": [
    "# Dataset build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a610ade6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioPairQualityDataset(Dataset):\n",
    "    def __init__(self, csv_path, transform=None, target_sample_rate=16000, duration=3.0):\n",
    "        self.data = pd.read_csv(csv_path, header=None, names=[\"source_path\", \"target_path\", \"quality_score\"])\n",
    "        self.transform = transform\n",
    "        self.sample_rate = target_sample_rate\n",
    "        self.num_samples = int(self.sample_rate * duration)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _load_audio(self, path):\n",
    "        waveform, sr = torchaudio.load(path)\n",
    "        # Resample if needed\n",
    "        if sr != self.sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=self.sample_rate)\n",
    "            waveform = resampler(waveform)\n",
    "        # Mono\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = waveform.mean(dim=0, keepdim=True)\n",
    "        # Pad/Trim to fixed length\n",
    "        if waveform.shape[1] < self.num_samples:\n",
    "            padding = self.num_samples - waveform.shape[1]\n",
    "            waveform = torch.nn.functional.pad(waveform, (0, padding))\n",
    "        else:\n",
    "            waveform = waveform[:, :self.num_samples]\n",
    "        return waveform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        source_audio = self._load_audio(row[\"source_path\"])\n",
    "        target_audio = self._load_audio(row[\"target_path\"])\n",
    "        score = torch.tensor(row[\"quality_score\"], dtype=torch.float32)\n",
    "\n",
    "        if self.transform:\n",
    "            source_audio = self.transform(source_audio)\n",
    "            target_audio = self.transform(target_audio)\n",
    "\n",
    "        return {\n",
    "            'source_audio': source_audio,\n",
    "            'target_audio': target_audio,\n",
    "            'score': score\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "551bb7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = os.path.join(EXTRACT_DIR, 'paired_audio_quality_dataset.csv')\n",
    "\n",
    "# Full dataset\n",
    "full_dataset = AudioPairQualityDataset(csv_path)\n",
    "\n",
    "# Define lengths for the splits\n",
    "total_len = len(full_dataset)\n",
    "train_len = int(0.7 * total_len)\n",
    "val_len = int(0.15 * total_len)\n",
    "test_len = total_len - train_len - val_len  # Assicura somma esatta\n",
    "\n",
    "# Random splits for train, test and validation\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    full_dataset,\n",
    "    [train_len, val_len, test_len],\n",
    "    generator=torch.Generator().manual_seed(42)  # seed riproducibile\n",
    ")\n",
    "\n",
    "# DataLoader per ciascun set\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d6dee166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader example:\n",
      "Spectrogram shape: torch.Size([32, 1, 48000]), Score shape: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# get one dataloader element\n",
    "print(\"Train loader example:\")\n",
    "for batch in train_loader:\n",
    "    spectrogram = batch['source_audio']\n",
    "    score = batch['score']\n",
    "    print(f\"Spectrogram shape: {spectrogram.shape}, Score shape: {score.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "07a97b87",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (48000,) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Mostra il primo spettrogramma del batch\u001b[39;00m\n\u001b[32m      2\u001b[39m plt.figure(figsize=(\u001b[32m10\u001b[39m, \u001b[32m4\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mplt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspectrogram\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maspect\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morigin\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlower\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m plt.title(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mScore: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore[\u001b[32m0\u001b[39m].item()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m plt.colorbar(label=\u001b[33m'\u001b[39m\u001b[33mAmplitude\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alex\\anaconda3\\envs\\audio_restore_project\\Lib\\site-packages\\matplotlib\\pyplot.py:3601\u001b[39m, in \u001b[36mimshow\u001b[39m\u001b[34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, colorizer, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[39m\n\u001b[32m   3579\u001b[39m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes.imshow)\n\u001b[32m   3580\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mimshow\u001b[39m(\n\u001b[32m   3581\u001b[39m     X: ArrayLike | PIL.Image.Image,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3599\u001b[39m     **kwargs,\n\u001b[32m   3600\u001b[39m ) -> AxesImage:\n\u001b[32m-> \u001b[39m\u001b[32m3601\u001b[39m     __ret = \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3602\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3603\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3604\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3605\u001b[39m \u001b[43m        \u001b[49m\u001b[43maspect\u001b[49m\u001b[43m=\u001b[49m\u001b[43maspect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3606\u001b[39m \u001b[43m        \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3607\u001b[39m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3608\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvmin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3609\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvmax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3610\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolorizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolorizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3611\u001b[39m \u001b[43m        \u001b[49m\u001b[43morigin\u001b[49m\u001b[43m=\u001b[49m\u001b[43morigin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3612\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3613\u001b[39m \u001b[43m        \u001b[49m\u001b[43minterpolation_stage\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterpolation_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3614\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilternorm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilternorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3615\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilterrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilterrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3616\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3617\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3618\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3619\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3620\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3621\u001b[39m     sci(__ret)\n\u001b[32m   3622\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m __ret\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alex\\anaconda3\\envs\\audio_restore_project\\Lib\\site-packages\\matplotlib\\__init__.py:1521\u001b[39m, in \u001b[36m_preprocess_data.<locals>.inner\u001b[39m\u001b[34m(ax, data, *args, **kwargs)\u001b[39m\n\u001b[32m   1518\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m   1519\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m(ax, *args, data=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m   1520\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1521\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[43m            \u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1523\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcbook\u001b[49m\u001b[43m.\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1524\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbook\u001b[49m\u001b[43m.\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1526\u001b[39m     bound = new_sig.bind(ax, *args, **kwargs)\n\u001b[32m   1527\u001b[39m     auto_label = (bound.arguments.get(label_namer)\n\u001b[32m   1528\u001b[39m                   \u001b[38;5;129;01mor\u001b[39;00m bound.kwargs.get(label_namer))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alex\\anaconda3\\envs\\audio_restore_project\\Lib\\site-packages\\matplotlib\\axes\\_axes.py:5979\u001b[39m, in \u001b[36mAxes.imshow\u001b[39m\u001b[34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, colorizer, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[39m\n\u001b[32m   5976\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m aspect \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5977\u001b[39m     \u001b[38;5;28mself\u001b[39m.set_aspect(aspect)\n\u001b[32m-> \u001b[39m\u001b[32m5979\u001b[39m \u001b[43mim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5980\u001b[39m im.set_alpha(alpha)\n\u001b[32m   5981\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m im.get_clip_path() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5982\u001b[39m     \u001b[38;5;66;03m# image does not already have clipping set, clip to Axes patch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alex\\anaconda3\\envs\\audio_restore_project\\Lib\\site-packages\\matplotlib\\image.py:685\u001b[39m, in \u001b[36m_ImageBase.set_data\u001b[39m\u001b[34m(self, A)\u001b[39m\n\u001b[32m    683\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(A, PIL.Image.Image):\n\u001b[32m    684\u001b[39m     A = pil_to_array(A)  \u001b[38;5;66;03m# Needed e.g. to apply png palette.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m685\u001b[39m \u001b[38;5;28mself\u001b[39m._A = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_normalize_image_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[38;5;28mself\u001b[39m._imcache = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    687\u001b[39m \u001b[38;5;28mself\u001b[39m.stale = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alex\\anaconda3\\envs\\audio_restore_project\\Lib\\site-packages\\matplotlib\\image.py:653\u001b[39m, in \u001b[36m_ImageBase._normalize_image_array\u001b[39m\u001b[34m(A)\u001b[39m\n\u001b[32m    651\u001b[39m     A = A.squeeze(-\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# If just (M, N, 1), assume scalar and apply colormap.\u001b[39;00m\n\u001b[32m    652\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (A.ndim == \u001b[32m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m A.ndim == \u001b[32m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m A.shape[-\u001b[32m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [\u001b[32m3\u001b[39m, \u001b[32m4\u001b[39m]):\n\u001b[32m--> \u001b[39m\u001b[32m653\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mA.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for image data\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m A.ndim == \u001b[32m3\u001b[39m:\n\u001b[32m    655\u001b[39m     \u001b[38;5;66;03m# If the input data has values outside the valid range (after\u001b[39;00m\n\u001b[32m    656\u001b[39m     \u001b[38;5;66;03m# normalisation), we issue a warning and then clip X to the bounds\u001b[39;00m\n\u001b[32m    657\u001b[39m     \u001b[38;5;66;03m# - otherwise casting wraps extreme values, hiding outliers and\u001b[39;00m\n\u001b[32m    658\u001b[39m     \u001b[38;5;66;03m# making reliable interpretation impossible.\u001b[39;00m\n\u001b[32m    659\u001b[39m     high = \u001b[32m255\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np.issubdtype(A.dtype, np.integer) \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m1\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: Invalid shape (48000,) for image data"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Mostra il primo spettrogramma del batch\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.imshow(spectrogram[0].squeeze(0).numpy(), aspect='auto', origin='lower')\n",
    "plt.title(f\"Score: {score[0].item():.2f}\")\n",
    "plt.colorbar(label='Amplitude')\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Mel bins\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f62bbb",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "411f4517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AudioQualityCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AudioQualityCNN, self).__init__()\n",
    "\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2)),  # [16, 64, 1200]\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2)),  # [32, 32, 600]\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2))   # [64, 16, 300]\n",
    "        )\n",
    "\n",
    "        self.fc_block = nn.Sequential(\n",
    "            nn.Flatten(),                      # [64 * 16 * 300] = 307200\n",
    "            nn.Linear(64 * 16 * 300, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 1)                  # Output: qualità\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block(x)\n",
    "        x = self.fc_block(x)\n",
    "        return x.squeeze(1)  # Output shape: [batch_size]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e2e877",
   "metadata": {},
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d1931bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = AudioQualityCNN().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.MSELoss()  # oppure nn.L1Loss() per MAE diretta\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "\n",
    "def train(model, train_loader, val_loader, epochs=20):\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 3\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    os.makedirs(\"model\", exist_ok=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_mae = 0.0\n",
    "\n",
    "        loop = tqdm(train_loader, total=len(train_loader))\n",
    "\n",
    "        for batch in loop:\n",
    "            inputs = batch['spectrogram'].to(device)\n",
    "            targets = batch['score'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # MAE per batch\n",
    "            batch_mae = torch.mean(torch.abs(outputs - targets)).item()\n",
    "            running_mae += batch_mae * inputs.size(0)\n",
    "\n",
    "            loop.set_description(f\"\\033[34mEpoch [{epoch+1}/{epochs}]\\033[0m\")\n",
    "            loop.set_postfix(loss=loss.item(), mae=batch_mae)\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader.dataset)\n",
    "        avg_train_mae = running_mae / len(train_loader.dataset)\n",
    "\n",
    "        scheduler.step(avg_train_loss)\n",
    "\n",
    "        tqdm.write(f\"\\033[34mEpoch [{epoch+1}/{epochs}]\\033[0m, \"\n",
    "                   f\"Train Loss: {avg_train_loss:.4f}, Train MAE: {avg_train_mae:.2f}\")\n",
    "\n",
    "        #### VALIDATION ####\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_mae = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs = batch['spectrogram'].to(device)\n",
    "                targets = batch['score'].to(device)\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                batch_mae = torch.mean(torch.abs(outputs - targets)).item()\n",
    "                val_mae += batch_mae * inputs.size(0)\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "        avg_val_mae = val_mae / len(val_loader.dataset)\n",
    "\n",
    "        tqdm.write(f\"\\033[32mValidation Loss: {avg_val_loss:.4f}, Validation MAE: {avg_val_mae:.2f}\\033[0m\")\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), \"model/best_model.pth\")\n",
    "            epochs_without_improvement = 0\n",
    "            tqdm.write(f\"\\033[32mModel saved!\\033[0m\")\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            tqdm.write(f\"\\033[31mNo improvement ({epochs_without_improvement}/{patience})\\033[0m\")\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            tqdm.write(f\"\\033[31mEarly stopping triggered\\033[0m\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e35e328",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34mEpoch [1/30]\u001b[0m: 100%|██████████| 6/6 [00:18<00:00,  3.11s/it, loss=587, mae=19.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mEpoch [1/30]\u001b[0m, Train Loss: 673.4465, Train MAE: 21.66\n",
      "\u001b[32mValidation Loss: 864.6388, Validation MAE: 26.59\u001b[0m\n",
      "\u001b[32mModel saved!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34mEpoch [2/30]\u001b[0m: 100%|██████████| 6/6 [00:18<00:00,  3.12s/it, loss=1.02e+3, mae=27.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mEpoch [2/30]\u001b[0m, Train Loss: 713.6727, Train MAE: 22.19\n",
      "\u001b[32mValidation Loss: 907.2527, Validation MAE: 26.66\u001b[0m\n",
      "\u001b[31mNo improvement (1/3)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34mEpoch [3/30]\u001b[0m: 100%|██████████| 6/6 [00:18<00:00,  3.11s/it, loss=602, mae=20.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mEpoch [3/30]\u001b[0m, Train Loss: 700.6142, Train MAE: 22.24\n",
      "\u001b[32mValidation Loss: 847.1121, Validation MAE: 25.48\u001b[0m\n",
      "\u001b[32mModel saved!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34mEpoch [4/30]\u001b[0m: 100%|██████████| 6/6 [00:18<00:00,  3.09s/it, loss=636, mae=22]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mEpoch [4/30]\u001b[0m, Train Loss: 666.6382, Train MAE: 21.42\n",
      "\u001b[32mValidation Loss: 841.0141, Validation MAE: 25.81\u001b[0m\n",
      "\u001b[32mModel saved!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34mEpoch [5/30]\u001b[0m: 100%|██████████| 6/6 [00:18<00:00,  3.11s/it, loss=577, mae=21.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mEpoch [5/30]\u001b[0m, Train Loss: 655.6476, Train MAE: 21.16\n",
      "\u001b[32mValidation Loss: 862.5699, Validation MAE: 25.92\u001b[0m\n",
      "\u001b[31mNo improvement (1/3)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34mEpoch [6/30]\u001b[0m: 100%|██████████| 6/6 [00:18<00:00,  3.12s/it, loss=1.28e+3, mae=28.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mEpoch [6/30]\u001b[0m, Train Loss: 677.8350, Train MAE: 21.50\n",
      "\u001b[32mValidation Loss: 922.6269, Validation MAE: 27.18\u001b[0m\n",
      "\u001b[31mNo improvement (2/3)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34mEpoch [7/30]\u001b[0m: 100%|██████████| 6/6 [00:18<00:00,  3.12s/it, loss=645, mae=22.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mEpoch [7/30]\u001b[0m, Train Loss: 647.8408, Train MAE: 21.41\n",
      "\u001b[32mValidation Loss: 919.3894, Validation MAE: 26.68\u001b[0m\n",
      "\u001b[31mNo improvement (3/3)\u001b[0m\n",
      "\u001b[31mEarly stopping triggered\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train(model, train_loader, val_loader, epochs=30)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio_restore_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
