{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88021362",
   "metadata": {},
   "source": [
    "# Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "19570464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device use: mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchaudio.transforms import Spectrogram, AmplitudeToDB\n",
    "from torch import hann_window\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DEVICE = (\"cuda\" if torch.cuda.is_available()\n",
    "          else \"mps\" if torch.backends.mps.is_available()\n",
    "          else \"cpu\")\n",
    "\n",
    "if DEVICE == 'cuda':\n",
    "    from torch.cuda.amp import autocast, GradScaler\n",
    "else:\n",
    "    from contextlib import nullcontext\n",
    "    autocast = nullcontext  \n",
    "    \n",
    "    class GradScaler:\n",
    "        def scale(self, loss): return loss\n",
    "        def step(self, optimizer): optimizer.step()\n",
    "        def update(self): pass\n",
    "        def __getattr__(self, name): return lambda *args, **kwargs: None\n",
    "\n",
    "BATCH_SIZE   = 8\n",
    "EPOCHS       = 1\n",
    "LEARNING_RATE= 5e-5\n",
    "SAMPLE_RATE  = 16000\n",
    "N_FFT        = 1024\n",
    "HOP_LENGTH   = 32     \n",
    "WIN_LENGTH   = N_FFT\n",
    "N_FRAMES     = 256\n",
    "CLEAR_DIR    = '../../data/train/clean'\n",
    "DEGRADED_DIR = '../../data/train/degraded'\n",
    "MODEL_SAVE   = '../../model/UNet_audio_restoration.pth'\n",
    "CHECKPOINT_DIR = '../../test'\n",
    "\n",
    "print(f'Device use: {DEVICE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba97a9df",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "AudioPairDataset is a class extending `torch.utils.data.Dataset` to handle pairs of “clean” and “degraded” audio files. It loads the signals, converts them to decibel-scale spectrograms, normalizes them, and ensures a fixed length via padding or cropping, returning a tuple ready for denoising model training.\n",
    "\n",
    "---\n",
    "\n",
    "## Constructor Parameters\n",
    "\n",
    "- **clean_dir**  \n",
    "  Directory containing clean audio files.  \n",
    "- **degraded_dir**  \n",
    "  Directory containing degraded audio files.  \n",
    "- **sample_rate** (default: 16000)  \n",
    "  Expected sampling rate (read from file by `torchaudio.load`).  \n",
    "- **n_fft** (default: 1024)  \n",
    "  FFT window size used for spectrogram computation.  \n",
    "- **hop_length** (default: 512)  \n",
    "  Hop length (in samples) between successive windows.  \n",
    "- **n_frames** (default: 256)  \n",
    "  Desired number of time frames per spectrogram, enforced via padding or cropping.\n",
    "\n",
    "### Internal Components\n",
    "\n",
    "- **File list**: stores sorted filenames from `clean_dir`.  \n",
    "- **Spectrogram**: instance computing a power spectrogram from raw signal.  \n",
    "- **AmplitudeToDB**: converts power spectrogram values to decibel scale.\n",
    "\n",
    "---\n",
    "\n",
    "#### `__len__`\n",
    "\n",
    "Returns the total number of examples (i.e., the number of files in the “clean” directory), enabling PyTorch DataLoader support for batching.\n",
    "\n",
    "---\n",
    "\n",
    "## Padding or Cropping Helper\n",
    "\n",
    "- Accepts a 3-D spectrogram tensor of shape `[channels, frequency_bins, time_frames]`.  \n",
    "- If `time_frames` < `n_frames`, pads with zeros at the end to reach the desired length.  \n",
    "- If `time_frames` ≥ `n_frames`, crops to keep only the first `n_frames`.\n",
    "\n",
    "---\n",
    "\n",
    "#### `__getitem__`\n",
    "\n",
    "1. Selects the filename at the given index.  \n",
    "2. Loads clean and degraded audio with `torchaudio.load`.  \n",
    "3. Converts to mono by averaging channels.  \n",
    "4. Computes power spectrogram and converts to decibel scale.  \n",
    "5. Applies z-score normalization (subtract mean, divide by standard deviation plus a small epsilon).  \n",
    "6. Pads or crops each spectrogram to ensure `n_frames`.  \n",
    "7. Returns a tuple `(degraded_spectrogram, clean_spectrogram)`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "947336fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioPairDataset(Dataset):\n",
    "    def __init__(self, clean_dir, degraded_dir,\n",
    "                 sample_rate=16000, n_fft=1024,\n",
    "                 hop_length=128, n_frames=256):\n",
    "        self.clean_dir    = clean_dir\n",
    "        self.degraded_dir = degraded_dir\n",
    "        self.files        = sorted(os.listdir(clean_dir))\n",
    "        self.n_frames     = n_frames\n",
    "        # Spectrogram senza window arg\n",
    "        self.spec  = Spectrogram(\n",
    "            n_fft=n_fft,\n",
    "            hop_length=hop_length,\n",
    "            win_length=n_fft,\n",
    "            power=2.0\n",
    "        )\n",
    "        self.to_db = AmplitudeToDB(stype='power')\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def pad_or_crop(self, S):\n",
    "        # S: [C, F, T]\n",
    "        C, freq, time = S.shape\n",
    "        if time < self.n_frames:\n",
    "            pad_amount = self.n_frames - time\n",
    "            return F.pad(S, (0, pad_amount))\n",
    "        else:\n",
    "            return S[:, : , :self.n_frames]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fn = self.files[idx]\n",
    "        cw, _ = torchaudio.load(os.path.join(self.clean_dir, fn))\n",
    "        dw, _ = torchaudio.load(os.path.join(self.degraded_dir, fn))\n",
    "        cw = cw.mean(0, keepdim=True)\n",
    "        dw = dw.mean(0, keepdim=True)\n",
    "        S_c = self.to_db(self.spec(cw))\n",
    "        S_d = self.to_db(self.spec(dw))\n",
    "        S_c = (S_c - S_c.mean()) / (S_c.std() + 1e-6)\n",
    "        S_d = (S_d - S_d.mean()) / (S_d.std() + 1e-6)\n",
    "        S_c = self.pad_or_crop(S_c)\n",
    "        S_d = self.pad_or_crop(S_d)\n",
    "        return S_d, S_c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eeef831",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "The UNet model is a fully convolutional neural network designed for image-to-image tasks. It consists of symmetric encoder and decoder paths with skip connections, allowing precise localization and context integration.\n",
    "\n",
    "---\n",
    "\n",
    "## Building Block: UNetBlock\n",
    "\n",
    "Each UNetBlock comprises two convolutional layers with the following sequence:\n",
    "- 3×3 convolution (padding=1)  \n",
    "- Instance normalization (affine)  \n",
    "- ReLU activation (in-place)  \n",
    "- 3×3 convolution (padding=1)  \n",
    "- Instance normalization (affine)  \n",
    "- ReLU activation (in-place)\n",
    "\n",
    "This block preserves spatial dimensions and refines feature representations.\n",
    "\n",
    "---\n",
    "\n",
    "## Architecture Components\n",
    "\n",
    "### Encoder Path\n",
    "\n",
    "1. **enc1**: UNetBlock mapping `in_channels → 64`  \n",
    "2. **pool**: 2×2 max pooling  \n",
    "3. **enc2**: UNetBlock mapping `64 → 128`  \n",
    "4. **pool**: 2×2 max pooling  \n",
    "5. **enc3**: UNetBlock mapping `128 → 256`  \n",
    "\n",
    "Each encoder stage reduces spatial resolution by half after the block via max pooling.\n",
    "\n",
    "### Bottleneck\n",
    "\n",
    "- **bottleneck**: UNetBlock mapping `256 → 512`  \n",
    "- Applied after the third pooling to capture the most abstract features.\n",
    "\n",
    "### Decoder Path\n",
    "\n",
    "1. **up3**: Transposed convolution `512 → 256` upsampling by factor 2  \n",
    "2. **dec3**: UNetBlock on concatenated `[upsampled, cropped enc3]` (`512 → 256`)  \n",
    "3. **up2**: Transposed convolution `256 → 128` upsampling by factor 2  \n",
    "4. **dec2**: UNetBlock on concatenated `[upsampled, cropped enc2]` (`256 → 128`)  \n",
    "5. **up1**: Transposed convolution `128 → 64` upsampling by factor 2  \n",
    "6. **dec1**: UNetBlock on concatenated `[upsampled, cropped enc1]` (`128 → 64`)  \n",
    "\n",
    "Skip connections fuse high-resolution encoder features with decoder upsampled maps for precise reconstruction.\n",
    "\n",
    "### Final Convolution\n",
    "\n",
    "- **final_conv**: 1×1 convolution mapping `64 → out_channels` to produce the output feature map.\n",
    "\n",
    "---\n",
    "\n",
    "## Center Crop Utility\n",
    "\n",
    "A helper function crops encoder feature maps to match decoder spatial dimensions before concatenation:\n",
    "- Computes offsets `(dh, dw)` from shape differences  \n",
    "- Returns centrally cropped tensor of target height and width\n",
    "\n",
    "---\n",
    "\n",
    "## Forward Pass Summary\n",
    "\n",
    "1. **Encoding**  \n",
    "   - e1 = enc1(x)  \n",
    "   - e2 = enc2(pool(e1))  \n",
    "   - e3 = enc3(pool(e2))  \n",
    "2. **Bottleneck**  \n",
    "   - b = bottleneck(pool(e3))  \n",
    "3. **Decoding with Skip Connections**  \n",
    "   - d3 = up3(b) → crop e3 to d3 size → dec3(cat(d3, e3_cropped))  \n",
    "   - d2 = up2(d3) → crop e2 to d2 size → dec2(cat(d2, e2_cropped))  \n",
    "   - d1 = up1(d2) → crop e1 to d1 size → dec1(cat(d1, e1_cropped))  \n",
    "4. **Output**  \n",
    "   - out = final_conv(d1)\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Notes\n",
    "\n",
    "- Instance normalization stabilizes training across batches of size one.  \n",
    "- Skip connections mitigate information loss and support fine detail reconstruction.  \n",
    "- The model is flexible in input/output channels and can be adapted to various image dimensions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "85189975",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.InstanceNorm2d(out_ch, affine=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.InstanceNorm2d(out_ch, affine=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x): return self.block(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_ch=1, out_ch=1):\n",
    "        super().__init__()\n",
    "        self.enc1 = UNetBlock(in_ch, 64)\n",
    "        self.enc2 = UNetBlock(64, 128)\n",
    "        self.enc3 = UNetBlock(128, 256)\n",
    "        self.bottleneck = UNetBlock(256, 512)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.up3 = nn.ConvTranspose2d(512, 256, 2, 2)\n",
    "        self.dec3 = UNetBlock(512, 256)\n",
    "        self.up2 = nn.ConvTranspose2d(256, 128, 2, 2)\n",
    "        self.dec2 = UNetBlock(256, 128)\n",
    "        self.up1 = nn.ConvTranspose2d(128, 64, 2, 2)\n",
    "        self.dec1 = UNetBlock(128, 64)\n",
    "        self.final_conv = nn.Conv2d(64, out_ch, 1)\n",
    "\n",
    "    def center_crop(self, src, tgt):\n",
    "        _,_,h,w = src.shape\n",
    "        _,_,th,tw = tgt.shape\n",
    "        dh, dw = (h-th)//2, (w-tw)//2\n",
    "        return src[:,:,dh:dh+th, dw:dw+tw]\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool(e1))\n",
    "        e3 = self.enc3(self.pool(e2))\n",
    "        b = self.bottleneck(self.pool(e3))\n",
    "        d3 = self.up3(b)\n",
    "        e3c = self.center_crop(e3, d3)\n",
    "        d3 = self.dec3(torch.cat([d3, e3c], dim=1))\n",
    "        d2 = self.up2(d3)\n",
    "        e2c = self.center_crop(e2, d2)\n",
    "        d2 = self.dec2(torch.cat([d2, e2c], dim=1))\n",
    "        d1 = self.up1(d2)\n",
    "        e1c = self.center_crop(e1, d1)\n",
    "        d1 = self.dec1(torch.cat([d1, e1c], dim=1))\n",
    "        return self.final_conv(d1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4198dd59",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "- **Checkpoint Directory**  \n",
    "  Creates `CHECKPOINT_DIR` if it doesn’t exist and initializes `best_val` to track the lowest validation loss.\n",
    "\n",
    "- **Dataset and DataLoader**  \n",
    "  - Instantiates `AudioPairDataset` with parameters for sample rate, FFT size, hop length and number of frames.  \n",
    "  - Splits the full dataset into 80 % training and 20 % validation.  \n",
    "  - Wraps each split in a `DataLoader` with a specified `BATCH_SIZE`, shuffling only the training loader.\n",
    "\n",
    "- **Model, Loss, Optimizer, Scheduler**  \n",
    "  - Builds a `UNet` model and moves it to the chosen `DEVICE`.  \n",
    "  - Uses `L1Loss` as the loss function.  \n",
    "  - Chooses the Adam optimizer with a specified learning rate.  \n",
    "  - Applies a `ReduceLROnPlateau` scheduler that halves the LR if validation loss does not improve for 5 epochs.\n",
    "\n",
    "---\n",
    "\n",
    "## Training Loop\n",
    "\n",
    "For each epoch from 1 to `EPOCHS`:\n",
    "\n",
    "1. **Training Phase**  \n",
    "   - Sets the model to train mode.  \n",
    "   - Iterates over the training loader, moving inputs and targets to `DEVICE`.  \n",
    "   - Performs a forward pass, cropping the clean target if its shape exceeds the model output.  \n",
    "   - Computes loss, backpropagates gradients, and updates model parameters.  \n",
    "   - Accumulates running loss and records the epoch’s average training loss.\n",
    "\n",
    "2. **Validation Phase**  \n",
    "   - Switches the model to evaluation mode and disables gradient computation.  \n",
    "   - Iterates over the validation loader, repeating the forward pass and any necessary cropping.  \n",
    "   - Accumulates validation loss and records the average validation loss.  \n",
    "   - Steps the LR scheduler with the current `avg_val` loss.\n",
    "\n",
    "3. **Checkpointing**  \n",
    "   - **Periodic Checkpoints**: Every 5 epochs, saves a full checkpoint (model & optimizer states, epoch, and validation loss) to `CHECKPOINT_DIR`.  \n",
    "   - **Best Model**: If the current `avg_val` is lower than `best_val`, updates `best_val`, and saves the model weights as `best.pth`.\n",
    "\n",
    "4. **Logging**  \n",
    "   - Prints epoch summary including training and validation losses.  \n",
    "   - Prints notifications when checkpoints or a new best model are saved.\n",
    "\n",
    "---\n",
    "\n",
    "## Final Save\n",
    "\n",
    "After all epochs, saves the final model weights to `MODEL_SAVE` and prints a confirmation message.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0e5b55bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "Train Ep1/1 on mps:   6%|▋         | 50/800 [03:56<59:00,  4.72s/it]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 63\u001b[0m\n\u001b[1;32m     60\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     61\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 63\u001b[0m     run_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     65\u001b[0m avg_train \u001b[38;5;241m=\u001b[39m run_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[1;32m     66\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(avg_train)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def spectral_convergence(x, y):\n",
    "    return torch.norm(y - x, p='fro') / torch.norm(y, p='fro')\n",
    "\n",
    "# Preparo dataset e dataloader\n",
    "ds = AudioPairDataset(CLEAR_DIR, DEGRADED_DIR,\n",
    "                      SAMPLE_RATE, N_FFT, HOP_LENGTH, N_FRAMES)\n",
    "n_val = int(0.2 * len(ds))\n",
    "train_ds, val_ds = random_split(ds, [len(ds)-n_val, n_val])\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=BATCH_SIZE,\n",
    "    shuffle=True,  num_workers=0, pin_memory=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds, batch_size=BATCH_SIZE,\n",
    "    shuffle=False, num_workers=0, pin_memory=True\n",
    ")\n",
    "\n",
    "# Modello, opt, scheduler, AMP\n",
    "model     = UNet().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
    ")\n",
    "use_amp = (DEVICE == 'cuda')\n",
    "scaler  = GradScaler() if use_amp else None\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "best_val = float('inf')\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    run_loss = 0.0\n",
    "\n",
    "    for deg, clean in tqdm(train_loader, desc=f\"Train Ep{epoch}/{EPOCHS} on {DEVICE}\"):\n",
    "        # deg, clean: [B,1,F,T]\n",
    "        deg = deg.to(DEVICE)\n",
    "        cl  = clean.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if use_amp:\n",
    "            with autocast():\n",
    "                out = model(deg)\n",
    "                if out.shape != cl.shape:\n",
    "                    cl = cl[..., :out.size(2), :out.size(3)]\n",
    "                l1 = F.l1_loss(torch.log1p(out), torch.log1p(cl))\n",
    "                sc = spectral_convergence(out, cl)\n",
    "                loss = l1 + 0.1 * sc\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            out = model(deg)\n",
    "            if out.shape != cl.shape:\n",
    "                cl = cl[..., :out.size(2), :out.size(3)]\n",
    "            l1 = F.l1_loss(torch.log1p(out), torch.log1p(cl))\n",
    "            sc = spectral_convergence(out, cl)\n",
    "            loss = l1 + 0.1 * sc\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        run_loss += loss.item()\n",
    "\n",
    "    avg_train = run_loss / len(train_loader)\n",
    "    train_losses.append(avg_train)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_run = 0.0\n",
    "    with torch.no_grad():\n",
    "        for deg, clean in val_loader:\n",
    "            deg = deg.to(DEVICE)\n",
    "            cl  = clean.to(DEVICE)\n",
    "            out = model(deg)\n",
    "            if out.shape != cl.shape:\n",
    "                cl = cl[..., :out.size(2), :out.size(3)]\n",
    "            l1 = F.l1_loss(torch.log1p(out), torch.log1p(cl))\n",
    "            sc = spectral_convergence(out, cl)\n",
    "            val_run += (l1 + 0.1 * sc).item()\n",
    "\n",
    "    avg_val = val_run / len(val_loader)\n",
    "    val_losses.append(avg_val)\n",
    "    scheduler.step(avg_val)\n",
    "\n",
    "    # Checkpoints\n",
    "    if epoch % 5 == 0:\n",
    "        torch.save({'model_state': model.state_dict()},\n",
    "                   os.path.join(CHECKPOINT_DIR, f'checkpoint_ep{epoch}.pth'))\n",
    "    if avg_val < best_val:\n",
    "        best_val = avg_val\n",
    "        torch.save(model.state_dict(),\n",
    "                   os.path.join(CHECKPOINT_DIR, 'best.pth'))\n",
    "\n",
    "    print(f\"Epoch {epoch}: Train Loss = {avg_train:.4f}, Val Loss = {avg_val:.4f}\")\n",
    "\n",
    "# Salva modello finale e plot\n",
    "torch.save(model.state_dict(), MODEL_SAVE)\n",
    "print(f\"✅ Model saved to {MODEL_SAVE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1046e635",
   "metadata": {},
   "source": [
    "# Plot\n",
    "\n",
    "This snippet generates a line chart to visualize the model’s performance over epochs:\n",
    "\n",
    "1. **Create a new figure**  \n",
    "   Initializes a fresh plotting canvas.\n",
    "\n",
    "2. **Plot training loss**  \n",
    "   - Uses `train_losses` list  \n",
    "   - Labels the curve as “Train Loss”\n",
    "\n",
    "3. **Plot validation loss**  \n",
    "   - Uses `val_losses` list  \n",
    "   - Labels the curve as “Val Loss”\n",
    "\n",
    "4. **Label axes**  \n",
    "   - X-axis: “Epoch”  \n",
    "   - Y-axis: “L1 Loss”\n",
    "\n",
    "5. **Add legend**  \n",
    "   Displays labels for both curves in the plot area.\n",
    "\n",
    "6. **Set title**  \n",
    "   Titles the chart “Training & Validation Loss” for context.\n",
    "\n",
    "7. **Render the plot**  \n",
    "   Calls `plt.show()` to display the figure in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4bc40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses,   label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('L1 Loss')\n",
    "plt.legend()\n",
    "plt.title('Training & Validation Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
