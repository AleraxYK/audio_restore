{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88021362",
   "metadata": {},
   "source": [
    "# Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19570464",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import stantment\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Declare device constant\n",
    "DEVICE = (\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 80\n",
    "LEARNING_RATE = 1e-4\n",
    "SAMPLE_RATE = 16000\n",
    "N_MELS = 128\n",
    "CLEAR_DIR = '../../data/train/clean'\n",
    "DEGRADED_DIR = '../../data/train/degraded'\n",
    "MODEL_SAVE = '../../model/UNet_audio_restoration.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba97a9df",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "This is the dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947336fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioPairDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, clean_dir, degraded_dir, sample_rate=16000, n_mels=128, n_frames=256):\n",
    "        self.clean_dir = clean_dir\n",
    "        self.degraded_dir = degraded_dir\n",
    "        self.filenames = sorted(os.listdir(clean_dir))\n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_frames = n_frames\n",
    "        self.n_mels = n_mels\n",
    "\n",
    "        # Trasformazioni\n",
    "        self.to_mel = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_fft=1024,\n",
    "            hop_length=512,\n",
    "            n_mels=n_mels\n",
    "        )\n",
    "        self.amplitude_to_db = torchaudio.transforms.AmplitudeToDB()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.filenames[idx]\n",
    "\n",
    "        # Percorsi file\n",
    "        clean_path = os.path.join(self.clean_dir, fname)\n",
    "        degraded_path = os.path.join(self.degraded_dir, fname)\n",
    "\n",
    "        # Carica waveform\n",
    "        clean_waveform, _ = torchaudio.load(clean_path)\n",
    "        degraded_waveform, _ = torchaudio.load(degraded_path)\n",
    "\n",
    "        # Converti a mono\n",
    "        clean_waveform = clean_waveform.mean(dim=0, keepdim=True)\n",
    "        degraded_waveform = degraded_waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "        # MelSpectrogram + dB\n",
    "        clean_mel = self.amplitude_to_db(self.to_mel(clean_waveform))\n",
    "        degraded_mel = self.amplitude_to_db(self.to_mel(degraded_waveform))\n",
    "\n",
    "        # Forza dimensione esatta (1, 128, n_frames)\n",
    "        def adjust_shape(tensor, target_shape):\n",
    "            c, f, t = tensor.shape\n",
    "            pad_t = target_shape[2] - t\n",
    "            if pad_t > 0:\n",
    "                tensor = torch.nn.functional.pad(tensor, (0, pad_t))\n",
    "            else:\n",
    "                tensor = tensor[:, :, :target_shape[2]]\n",
    "            return tensor\n",
    "\n",
    "        target_shape = (1, self.n_mels, self.n_frames)\n",
    "        clean_mel = adjust_shape(clean_mel, target_shape)\n",
    "        degraded_mel = adjust_shape(degraded_mel, target_shape)\n",
    "\n",
    "        return degraded_mel, clean_mel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eeef831",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "This is the model part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85189975",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size = 3, padding = 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size = 3, padding = 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace = True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "        \n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels = 1, out_channels = 1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.enc1 = UNetBlock(in_channels, 64)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.enc2 = UNetBlock(64, 128)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.enc3 = UNetBlock(128, 256)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.bottleneck = UNetBlock(256, 512)\n",
    "\n",
    "        self.up3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.dec3 = UNetBlock(512, 256)\n",
    "        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.dec2 = UNetBlock(256, 128)\n",
    "        self.up1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.dec1 = UNetBlock(128, 64)\n",
    "\n",
    "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool1(e1))\n",
    "        e3 = self.enc3(self.pool2(e2))\n",
    "\n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(self.pool3(e3))\n",
    "\n",
    "        # Decoder with skip connections\n",
    "        d3 = self.up3(b)\n",
    "        d3 = self.dec3(torch.cat([d3, e3], dim=1))\n",
    "        d2 = self.up2(d3)\n",
    "        d2 = self.dec2(torch.cat([d2, e2], dim=1))\n",
    "        d1 = self.up1(d2)\n",
    "        d1 = self.dec1(torch.cat([d1, e1], dim=1))\n",
    "\n",
    "        # Output\n",
    "        out = self.final_conv(d1)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4198dd59",
   "metadata": {},
   "source": [
    "# Training\n",
    "Training part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5b55bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AudioPairDataset(\n",
    "    clean_dir = CLEAR_DIR, \n",
    "    degraded_dir = DEGRADED_DIR,\n",
    "    sample_rate = SAMPLE_RATE,\n",
    "    n_mels = N_MELS\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True)\n",
    "\n",
    "model = UNet(in_channels = 1, out_channels = 1).to(DEVICE)\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "\n",
    "loss_history = []\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    loop = tqdm(train_loader, desc = f\"Epoch {epoch + 1}/{EPOCHS}\", leave = True)\n",
    "\n",
    "    for degraded, clean in loop:\n",
    "        degraded, clean = degraded.to(DEVICE), clean.to(DEVICE)\n",
    "\n",
    "        output = model(degraded)\n",
    "        loss = criterion(output, clean)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        loop.set_postfix(loss = loss.item())\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    loss_history.append(avg_loss)\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), MODEL_SAVE)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1046e635",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4bc40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, EPOCHS + 1), loss_history, marker='o')\n",
    "plt.title(\"Training Loss per Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"L1 Loss\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
