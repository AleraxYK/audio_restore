# HOW TO RUN IT
Firstly, install the conda env provided by the environment.yaml file. To run correctly the code, we have to consider 4 main files:
<pre>
<code>
📁 audio-restore/
├── 📂 quality_estimator_code/
│   └── 📂 python/
│       └── estimate_quality.py
├── 📂 restore_code/
│   └── 📂 python/
│       └── start.py
├── music_gen.py
├── text_to_music_eval_prompts.csv
</code>
</pre>

On the csv file, you can add all the prompts from which you want to generate music, then run music_gen.py.

In start.py you have to pay attention on 2 constants:
- INPUT_FILE: music_gen generated audio path
- OUTPUT_FIN: restored audio path

So write correctly these 2 paths. 

Then you only have to execute the following command:
```bash
python -m quality_estimator_code.python.estimate_quality
```
It will try to restore the INPUT_FILE and then it will estimate a quality score.


# 🎧 Audio Restoration for Generative Models

This repository implements a modular enhancement pipeline designed to improve the perceptual quality of audio generated by text-to-music models such as MusicGen. While these models are capable of generating coherent musical ideas from text prompts, their outputs often suffer from compression artifacts, low bit depth, and a lack of high-frequency fidelity. Our pipeline addresses this gap by introducing a U-Net-based restoration module and a CNN-based perceptual quality estimator.

---

## 🧠 Abstract

Recent generative audio models like MusicGen produce impressive musical samples directly from text. However, these generations often suffer from low bit depth, quantization artifacts, noise, and a general lack of high-fidelity detail compared to studio-quality audio. 

In this project, we propose a novel approach to enhance the perceptual quality of generated audio. We employ a U-Net architecture to perform post-processing on the audio, removing artifacts and restoring acoustic richness. Additionally, we introduce a CNN-based quality assessment module trained to predict perceptual audio quality. This module provides feedback that guides the U-Net during training. Our results show measurable improvements both in objective metrics and subjective listening tests.

---

## 🗂️ Repository Structure

<pre>
<code>
📁 audio-restore/
├── 📂 restore_code/
│   ├── 📂 Jupyter/
│   │   ├── convert_wav_audio.ipynb        # Converts MP3 to mono WAV
│   │   ├── downsample_song.ipynb          # Simulates degradation (downsampling, noise)
│   │   └── audio_restore_train.ipynb      # Trains U-Net on degraded/clean waveform pairs
│   └── 📂 python/
│       └── start.py                       # Restore audio quality
├── 📂 quality_estimator_code/
│   ├── 📂 Jupyter/
│   │   ├── Discriminator.ipynb            # Trains CNN quality predictor on spectrograms
│   └── 📂 python/
│       └── estimate_quality.py            # Launches the full pipeline
├── music_gen.py                           # Wraps MusicGen inference from prompts
├── 📂 data/
│   └── 📂 train/                          # Example dataset (Free Music Archive subset)
├── 📂 models/                             # Saved weights for U-Net and CNN
</code>
</pre>
---

## 🔩 Installation
Install dependencies with:
```bash
conda env create -f environment.yaml
```
---

## 💻 System Overview

The pipeline consists of the following components:

1. **Music Generation**
   - Uses a pre-trained model (e.g., MusicGen) to produce audio from text.

2. **Audio Degradation**
   - Downsamples and alters generated audio to simulate poor fidelity.

3. **U-Net Restoration**
   - Enhances low-quality waveform using an encoder-decoder network.

4. **CNN Evaluation**
   - Predicts perceptual quality of restored audio using spectrograms.

5. **Loss Functions**
   - Combined L1 loss and CNN-based perceptual loss guide training.


---

## Experimental Setup

- Dataset: MusicGen outputs (10s, 32kHz)
- Degraded: Downsampled to 8kHz, then restored
- Train/Val split: 80/20
- Optimizer: Adam
- Epochs: 80
- Learning Rate: 5e-5

---

## 📊 Evaluation Metrics

- SNR (Signal-to-Noise Ratio): measures denoising
- LSD (Log Spectral Distance): frequency-domain fidelity
- CQS (CNN Quality Score): learned perceptual score
- MOS (Mean Opinion Score): subjective test on 10 participants

---

## Results

| Method              |  CQS   |  MOS   |
|---------------------|--------|--------|
| MusicGen (baseline) | 0.991  | 0.871  |
| U-Net Enhanced      | 0.998  | 0.893  |

| Method              |  SNR   |  LSD   |
|---------------------|--------|--------|
| U-Net Enhanced      | 4.757  | 0.246  |

---

## Authors

- Paul Alexandru Radu Loghin (raduloghin.1942544@di.uniroma1.it)
- Andrea Maggiore (maggiore.1947898@di.uniroma1.it)

