# 🎧 Audio Restoration for Generative Models

This repository implements a modular enhancement pipeline designed to improve the perceptual quality of audio generated by text-to-music models such as MusicGen. While these models are capable of generating coherent musical ideas from text prompts, their outputs often suffer from compression artifacts, low bit depth, and a lack of high-frequency fidelity. Our pipeline addresses this gap by introducing a U-Net-based restoration module and a CNN-based perceptual quality estimator.

---

## 🧠 Abstract

Recent generative audio models like MusicGen produce impressive musical samples directly from text. However, these generations often suffer from low bit depth, quantization artifacts, noise, and a general lack of high-fidelity detail compared to studio-quality audio. 

In this project, we propose a novel approach to enhance the perceptual quality of generated audio. We employ a U-Net architecture to perform post-processing on the audio, removing artifacts and restoring acoustic richness. Additionally, we introduce a CNN-based quality assessment module trained to predict perceptual audio quality. This module provides feedback that guides the U-Net during training. Our results show measurable improvements both in objective metrics and subjective listening tests.

---

## 🗂️ Repository Structure

<pre>
<code>
📁 audio-restore/
├── 📂 code/
│   ├── 📂 Jupyter/
│   │   ├── convert_wav_audio.ipynb        # Converts MP3 to mono WAV
│   │   ├── downsample_song.ipynb          # Simulates degradation (downsampling, noise)
│   │   └── audio_restore_train.ipynb      # Trains U-Net on degraded/clean waveform pairs
│   └── 📂 python/
│       └── start.py                        # Launches the full pipeline
├── Discriminator.ipynb                     # Trains CNN quality predictor on spectrograms
├── music_gen.py                            # Wraps MusicGen inference from prompts
├── 📂 data/
│   └── 📂 train/                            # Example dataset (Free Music Archive subset)
├── 📂 models/                               # Saved weights for U-Net and CNN
</code>
</pre>
---

## 🔩 Installation

Requirements:
- Python >= 3.8
- torch
- torchaudio
- librosa
- matplotlib
- scikit-learn
- pydub
- tqdm

Install dependencies with:
```bash
pip install torch torchaudio librosa matplotlib pydub scikit-learn tqdm
```
---

## 💻 System Overview

The pipeline consists of the following components:

1. **Music Generation**
   - Uses a pre-trained model (e.g., MusicGen) to produce audio from text.

2. **Audio Degradation**
   - Downsamples and alters generated audio to simulate poor fidelity.

3. **U-Net Restoration**
   - Enhances low-quality waveform using an encoder-decoder network.

4. **CNN Evaluation**
   - Predicts perceptual quality of restored audio using spectrograms.

5. **Loss Functions**
   - Combined L1 loss and CNN-based perceptual loss guide training.

---

## Running the Pipeline

1. Generate raw audio (or use provided samples)

python music_gen.py --prompts_file prompts.txt --output_dir data/generated/

2. Convert audio to WAV and downsample it

Run:
- convert_wav_audio.ipynb
- downsample_song.ipynb

3. Train U-Net

Open audio_restore_train.ipynb and execute the training cells.

4. Train CNN Quality Estimator

Open Discriminator.ipynb and follow the instructions to create the CQS model.

5. Launch the full pipeline

python start.py --input_dir data/generated/ --output_dir output/ --epochs 100

---

## Experimental Setup

- Dataset: MusicGen outputs (10s, 32kHz)
- Degraded: Downsampled to 8kHz, then restored
- Train/Val split: 80/20
- Optimizer: Adam
- Epochs: 100
- Learning Rate: 1e-4

---

## 📊 Evaluation Metrics

- SNR (Signal-to-Noise Ratio): measures denoising
- LSD (Log Spectral Distance): frequency-domain fidelity
- CQS (CNN Quality Score): learned perceptual score
- MOS (Mean Opinion Score): subjective test on 10 participants

---

## Results

| Method              | SNR   | LSD  | CQS  | MOS  |
|---------------------|-------|------|------|------|
| MusicGen (baseline) | 12.4  | 1.83 | 2.1  | 2.7  |
| U-Net Enhanced      | 17.6  | 1.21 | 3.5  | 3.9  |

---

## U-Net Architecture

- 1D convolutional encoder-decoder
- Skip connections for detail preservation
- Input: 10s mono waveform
- Loss: L1 + perceptual loss (from CNN)

---

## CNN Quality Estimator

- Input: log-mel spectrogram
- Output: scalar quality score
- Target: SNR-based heuristics + manual annotations
- Used as loss term during U-Net training

---

## Listening Tests

Ten human participants rated generated and enhanced samples (1–5 scale). Enhanced audio received significantly higher scores in terms of clarity, detail, and overall quality.

---

## Authors

- Paul Alexandru Radu Loghin (raduloghin.1942544@di.uniroma1.it)
- Andrea Maggiore (maggiore.1947898@di.uniroma1.it)

