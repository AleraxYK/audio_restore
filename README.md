# HOW TO RUN IT
Firstly, install the conda env provided by the environment.yaml file. To run correctly the code, we have to consider 4 main files:
<pre>
<code>
ğŸ“ audio-restore/
â”œâ”€â”€ ğŸ“‚ quality_estimator_code/
â”‚   â””â”€â”€ ğŸ“‚ python/
â”‚       â””â”€â”€ estimate_quality.py
â”œâ”€â”€ ğŸ“‚ restore_code/
â”‚   â””â”€â”€ ğŸ“‚ python/
â”‚       â””â”€â”€ start.py
â”œâ”€â”€ music_gen.py
â”œâ”€â”€ text_to_music_eval_prompts.csv
</code>
</pre>

On the csv file, you can add all the prompts from which you want to generate music, then run music_gen.py.

In start.py you have to pay attention on 2 constants:
- INPUT_FILE: music_gen generated audio path
- OUTPUT_FIN: restored audio path

So write correctly these 2 paths. 

Then you only have to execute the following command:
```bash
python -m quality_estimator_code.python.estimate_quality
```
It will try to restore the INPUT_FILE and then it will estimate a quality score.


# ğŸ§ Audio Restoration for Generative Models

This repository implements a modular enhancement pipeline designed to improve the perceptual quality of audio generated by text-to-music models such as MusicGen. While these models are capable of generating coherent musical ideas from text prompts, their outputs often suffer from compression artifacts, low bit depth, and a lack of high-frequency fidelity. Our pipeline addresses this gap by introducing a U-Net-based restoration module and a CNN-based perceptual quality estimator.

---

## ğŸ§  Abstract

Recent generative audio models like MusicGen produce impressive musical samples directly from text. However, these generations often suffer from low bit depth, quantization artifacts, noise, and a general lack of high-fidelity detail compared to studio-quality audio. 

In this project, we propose a novel approach to enhance the perceptual quality of generated audio. We employ a U-Net architecture to perform post-processing on the audio, removing artifacts and restoring acoustic richness. Additionally, we introduce a CNN-based quality assessment module trained to predict perceptual audio quality. This module provides feedback that guides the U-Net during training. Our results show measurable improvements both in objective metrics and subjective listening tests.

---

## ğŸ—‚ï¸ Repository Structure

<pre>
<code>
ğŸ“ audio-restore/
â”œâ”€â”€ ğŸ“‚ restore_code/
â”‚   â”œâ”€â”€ ğŸ“‚ Jupyter/
â”‚   â”‚   â”œâ”€â”€ convert_wav_audio.ipynb        # Converts MP3 to mono WAV
â”‚   â”‚   â”œâ”€â”€ downsample_song.ipynb          # Simulates degradation (downsampling, noise)
â”‚   â”‚   â””â”€â”€ audio_restore_train.ipynb      # Trains U-Net on degraded/clean waveform pairs
â”‚   â””â”€â”€ ğŸ“‚ python/
â”‚       â””â”€â”€ start.py                       # Restore audio quality
â”œâ”€â”€ ğŸ“‚ quality_estimator_code/
â”‚   â”œâ”€â”€ ğŸ“‚ Jupyter/
â”‚   â”‚   â”œâ”€â”€ Discriminator.ipynb            # Trains CNN quality predictor on spectrograms
â”‚   â””â”€â”€ ğŸ“‚ python/
â”‚       â””â”€â”€ estimate_quality.py            # Launches the full pipeline
â”œâ”€â”€ music_gen.py                           # Wraps MusicGen inference from prompts
â”œâ”€â”€ ğŸ“‚ data/
â”‚   â””â”€â”€ ğŸ“‚ train/                          # Example dataset (Free Music Archive subset)
â”œâ”€â”€ ğŸ“‚ models/                             # Saved weights for U-Net and CNN
</code>
</pre>
---

## ğŸ”© Installation
Install dependencies with:
```bash
conda env create -f environment.yaml
```
---

## ğŸ’» System Overview

The pipeline consists of the following components:

1. **Music Generation**
   - Uses a pre-trained model (e.g., MusicGen) to produce audio from text.

2. **Audio Degradation**
   - Downsamples and alters generated audio to simulate poor fidelity.

3. **U-Net Restoration**
   - Enhances low-quality waveform using an encoder-decoder network.

4. **CNN Evaluation**
   - Predicts perceptual quality of restored audio using spectrograms.

5. **Loss Functions**
   - Combined L1 loss and CNN-based perceptual loss guide training.


---

## Experimental Setup

- Dataset: MusicGen outputs (10s, 32kHz)
- Degraded: Downsampled to 8kHz, then restored
- Train/Val split: 80/20
- Optimizer: Adam
- Epochs: 80
- Learning Rate: 5e-5

---

## ğŸ“Š Evaluation Metrics

- SNR (Signal-to-Noise Ratio): measures denoising
- LSD (Log Spectral Distance): frequency-domain fidelity
- CQS (CNN Quality Score): learned perceptual score
- MOS (Mean Opinion Score): subjective test on 10 participants

---

## Results

| Method              |  CQS   |  MOS   |
|---------------------|--------|--------|
| MusicGen (baseline) | 0.991  | 0.871  |
| U-Net Enhanced      | 0.998  | 0.893  |

| Method              |  SNR   |  LSD   |
|---------------------|--------|--------|
| U-Net Enhanced      | 4.757  | 0.246  |

---

## Authors

- Paul Alexandru Radu Loghin (raduloghin.1942544@di.uniroma1.it)
- Andrea Maggiore (maggiore.1947898@di.uniroma1.it)

